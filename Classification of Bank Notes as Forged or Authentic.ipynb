{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d81acfb",
   "metadata": {},
   "source": [
    "# Classification of Bank Notes as Forged or Authentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy.linalg as lin\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97600351",
   "metadata": {},
   "source": [
    "Perceptron Algorithm\n",
    "\n",
    "It is a supervised learning algorithm\n",
    "Basically it is a Binary Classification\n",
    "\n",
    "algorithm:\n",
    "\n",
    "step1: initialize w=w(0)\n",
    "\n",
    "step2: pick some random (x_n,y_n)\n",
    "\n",
    "step 3:if current w makes a mistake on (x_n, y_n) i.e., y_n * w(t)^T * x_n<0\n",
    "       \n",
    "                                          W_new = w(t) + y_n * x_n\n",
    "                                          \n",
    "step4: if not converged go to step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e133c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron\n",
    "#this is the perceptron function here the inputs are \n",
    "#the the training data, initial weights and number of iterations\n",
    "#and returns weight vector w\n",
    "def perceptron(data, init_, n_iter):\n",
    "    w = init_    #initializing the weight vector\n",
    "    for i in range(n_iter):\n",
    "#selection of data randomly from the dataset \n",
    "        random_sample = data[np.random.choice(data.shape[0], size=1, replace=False)][0]\n",
    "    #breaking into features and y\n",
    "        X, y = random_sample[:-1], random_sample[-1]\n",
    "        #checking if y*w.x is less than zero\n",
    "        if(y*np.dot(np.transpose(w), X) < 0):\n",
    "            #if the above condition satisfies then upgrade the weight\n",
    "            w += y*X\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b06777",
   "metadata": {},
   "source": [
    "Functions to make predictions using the algorithm for the banknotes dataset and Preprocessing the dataset to handle missing and anomalous data.\n",
    "\n",
    "To make the predictions we can check using sign(w(t)^T * x_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6edc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('BankNote_Authentication.csv')\n",
    "\n",
    "# checking if there are any null data in the dataset\n",
    "z= dataset.isnull().sum()\n",
    "#filling the empty values using mean of the respective feature\n",
    "missing_col = ['variance']\n",
    "for i in missing_col:\n",
    " dataset.loc[dataset.loc[:,i].isnull(),i]=dataset.loc[:,i].mean()\n",
    "missing_col = ['skewness']\n",
    "for i in missing_col:\n",
    " dataset.loc[dataset.loc[:,i].isnull(),i]=dataset.loc[:,i].mean()\n",
    "missing_col = ['curtosis']\n",
    "for i in missing_col:\n",
    " dataset.loc[dataset.loc[:,i].isnull(),i]=dataset.loc[:,i].mean()\n",
    "missing_col = ['entropy']\n",
    "for i in missing_col:\n",
    " dataset.loc[dataset.loc[:,i].isnull(),i]=dataset.loc[:,i].mean() \n",
    "z1= dataset.isnull().sum()\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Converting the daataframe to numpy \n",
    "dataset =df.to_numpy()\n",
    "np.random.shuffle(dataset)\n",
    "le = dataset.shape[0] #finding the total number of columns \n",
    "w =[1,1,1,1]\n",
    "#converting 0s into -1s as the perceptron algo deals with the dataset with classes 1 and -1\n",
    "for i in range(0,le): \n",
    "    if dataset[i,-1] == 0:\n",
    "        dataset[i,-1] = -1\n",
    "\n",
    "\n",
    "#predicting the data\n",
    "#for predicting the data need to check if w.x is positive or negative \n",
    "# if its positive then the data belongs to +1 class or it is -1 \n",
    "def predict(x,w):\n",
    "    #finding the number of rows in the input\n",
    "    l = x.shape[0]\n",
    "    #initializing the ypred \n",
    "    ypred =np.zeros(l)\n",
    "    #checking if w.x is positive or negative \n",
    "    for i in range(0,l):\n",
    "        p = np.dot(w, x[i,:])\n",
    "        if p < 0:\n",
    "            ypred[i] = -1\n",
    "        elif p>=0:\n",
    "            ypred[i] = 1\n",
    "    return ypred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1792b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for making the confusion matrix\n",
    "def confusion(a,p):\n",
    "    #finding the length of l\n",
    "    l = a.shape[0]\n",
    "    #initialize the value of m\n",
    "    m =np.zeros((2,2))\n",
    "    \n",
    "    for i in range(0,l):\n",
    "        if a[i] ==p[i]==-1:   # true positive\n",
    "            m[0,0] = m[0,0] +1\n",
    "        elif a[i] ==p[i]==1:   #true negative\n",
    "            m[1,1] = m[1,1] +1\n",
    "        elif a[i] ==1 and p[i]==-1:  #false negative\n",
    "            m[1,0] = m[1,0] +1   \n",
    "        elif a[i] ==-1 and p[i]==1:   #false positive\n",
    "            m[0,1] = m[0,1] +1  \n",
    "            \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3b7ec",
   "metadata": {},
   "source": [
    "Training the algorithm on the dataset using cross-validation and report cross-validated test set error\n",
    "\n",
    "To execute the cross validation the following algorithm is used:\n",
    "\n",
    "1. The entire dataset is divided into 20 equal sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffefc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(k):\n",
    "    \n",
    "    l = dataset.shape[0]\n",
    "    n =20\n",
    "    a= math.trunc((l/n))\n",
    "    predi = np.zeros(a)\n",
    "    accu = np.zeros(n)\n",
    "    # here p is used to make cross validation \n",
    "    for p in range(1,n+1):\n",
    "        # cross validation breaking a dataset of total rows into 20 equal parts and assigning 1 part for validation set for each iteration and\n",
    "        # finding the error\n",
    "        #breaking the dataset into features and y\n",
    "        testx = dataset[(p-1)*a: p*a, :-1]\n",
    "        testy = dataset[(p-1)*a: p*a,  -1]       \n",
    "        traind = np.delete(dataset, list(range((p-1)*a, p*a)), 0)       \n",
    "        weight = perceptron(traind,w,k)   #fiinding out the weight vector\n",
    "        predi = predict(testx,weight)     #finding out the predicted vector\n",
    "\n",
    "        conf = confusion(testy,predi)     #making the confusion matrix\n",
    "        #finding out the error percentage that is= (total no of wrongs )/ (total  no of dataset)        \n",
    "        accu[p-1] = (conf[0,1] + conf[1,0]) /( conf[0,0] + conf[1,0] + conf[0,1] + conf[1,1])  \n",
    "        \n",
    "        \n",
    "    ans = np.mean(accu)   #finding the mean of the errors\n",
    "        \n",
    "        \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c224ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f9cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for finding p, r and f1 values\n",
    "def f1(c):\n",
    "    p = (c[0,0])/(c[0,0] + c[1,0])     #finding the precision using confusion matrix\n",
    "    r = (c[0,0])/(c[0,0] + c[0,1])     #finding the recall using confusion matrix\n",
    "    \n",
    "    f = 2*p*r/(p+r)                    # finding f1 value using precision and recall\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470bbda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96875\n"
     ]
    }
   ],
   "source": [
    "# held out validation set with 80-20 percentage of training and test data respectively and use 550 as the number of iterations\n",
    "traind = dataset[0: math.trunc(0.8*(le)),:]  # importing 80 percentage of the data from the total as training data\n",
    "testx = dataset[:math.trunc(0.2*(le))  ,:-1] # importing 20 percentage of the data from the total as test data\n",
    "testy = dataset[:math.trunc(0.2*(le))  ,-1]\n",
    "w =[1,1,1,1]                          # initializing the weight vector\n",
    "weightq1d = perceptron(traind,w,550)     #finding weight using perceptron algo\n",
    "predi = predict(testx,weightq1d)      #predicting values using predict function \n",
    "conf = confusion(testy,predi)     #finding confusion matrix \n",
    "\n",
    "print(f1(conf))   # finding f1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e416a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26f5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
